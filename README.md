# Crypto ETL Pipeline

This repository contains a simple **ETL (Extract, Transform, Load) pipeline** for cryptocurrency data built with **Python** and automated using **GitHub Actions**.

The pipeline is designed to:

* Fetch cryptocurrency data from an external API
* Clean and transform raw data
* Store processed results in a local `data/` folder
* Run automatically on a schedule or manually on demand

---

## ğŸ“‚ Repository Structure

```text
data-pipeline-project/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ etl_pipeline.yml      # GitHub Actions workflow
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py                 # Configuration (API, parameters, etc.)
â”œâ”€â”€ data/                          # ETL output (generated files)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract.py                # Extract data from API
â”‚   â”œâ”€â”€ transform.py              # Data cleaning & transformation
â”‚   â””â”€â”€ load.py                   # Load / save processed data
â”œâ”€â”€ venv/                          # Virtual environment (not committed)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt (optional)
```

---

## âš™ï¸ Pipeline Overview

1. **Extract**
   Retrieves cryptocurrency data (e.g. price, volume, market cap) from an API.

2. **Transform**
   Cleans the data, normalizes formats, and prepares the final dataset.

3. **Load**
   Saves the transformed data into the `data/` directory (CSV / JSON).

The pipeline can be executed:

* Automatically every **6 hours** (cron schedule)
* Manually via **GitHub Actions â†’ Run workflow**

---

## ğŸ¤– GitHub Actions Workflow

The main workflow file is located at:

```
.github/workflows/etl_pipeline.yml
```

### Triggers

```yaml
on:
  schedule:
    - cron: "0 */6 * * *"   # every 6 hours
  workflow_dispatch:
```

---

## ğŸ” Auto Commit (ON / OFF Toggle)

This pipeline supports **optional automatic commits** of ETL results, which can be **enabled or disabled using a single toggle**.

### Configuration

In the workflow file:

```yaml
env:
  ENABLE_AUTO_COMMIT: "false"
```

| Value     | Behavior                                     |
| --------- | -------------------------------------------- |
| `"false"` | âŒ No automatic commit (default, recommended) |
| `"true"`  | âœ… Automatically commits the `data/` folder   |

### Why is it disabled by default?

* Scheduled ETL jobs often **produce no new data**
* Prevents unnecessary GitHub Actions failures
* Avoids excessive automated commits

---

## â–¶ï¸ Run Locally

### 1. Clone the repository

```bash
git clone https://github.com/<username>/<repo-name>.git
cd data-pipeline-project
```

### 2. Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate   # macOS/Linux
venv\\Scripts\\activate      # Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

(Or install manually: `pandas`, `requests`, `matplotlib`)

### 4. Run the ETL pipeline

```bash
python scripts/extract.py
python scripts/transform.py
python scripts/load.py
```

Processed output will be generated in the `data/` directory.

---

## ğŸ§¹ Git & Repository Rules

### Ignored by Git

* `venv/`
* `__pycache__/`
* Temporary and OS-specific files

### Not recommended to commit

* Large ETL output files (CSV / JSON)
* Credentials or API keys

---

## ğŸ—‘ï¸ Cleaning Up Workflow Runs

If workflow runs accumulate, they can be removed using **GitHub CLI**:

```bash
gh run list
gh run delete <RUN_ID>
```

Bulk delete example (Git Bash):

```bash
for id in $(gh run list --limit 1000 --json databaseId --jq '.[].databaseId'); do
  gh run delete $id
done
```

---

## ğŸ“Œ Important Notes

* Deleted workflow runs **cannot be restored**
* Auto-commit should be enabled only when necessary
* For production use, consider storing data in:

  * Cloud storage (S3 / GCS)
  * A database
  * GitHub Actions Artifacts

---

## âœ¨ Future Improvements

* Upload ETL results as **artifacts**
* Persist data to a database
* Add logging & monitoring
* Notifications (Slack / Email)

---

## ğŸ“„ License

This project is intended for learning and development purposes.

---

ğŸš€ **Happy data engineering!**


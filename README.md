# ğŸš€ Cryptocurrency Data Pipeline (JSON-Based ETL)

A complete **end-to-end data pipeline project** that extracts cryptocurrency market data from a public API, transforms raw JSON into clean analytical datasets, and serves the results through an interactive **Streamlit dashboard**.

This project is designed as a **portfolio-ready Data Engineering project**, showcasing modern ETL practices, automation with **GitHub Actions**, and a clean, reproducible project structure.

## ğŸ“Œ Project Overview

This repository implements a **three-stage ETL pipeline**:

- **Extract** â€” Collect raw cryptocurrency data from a public API  
- **Transform** â€” Clean, normalize, and structure JSON data  
- **Load** â€” Produce analytics-ready datasets for visualization  

The pipeline is fully automated using **GitHub Actions**, and the final output is consumed by a **Streamlit dashboard** for interactive exploration.

## ğŸ§± Project Structure

```text
data-pipeline-project/
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ etl_pipeline.yml      Automated ETL workflow (GitHub Actions)
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.yaml         Centralized configuration
â”‚
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ dashboard.py          Streamlit dashboard logic
â”‚   â””â”€â”€ app.py                Streamlit entry point
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  Raw JSON data (source of truth)
â”‚   â”œâ”€â”€ processed/            Cleaned & structured data
â”‚   â””â”€â”€ final/                Final analytics-ready dataset
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_extract_data.ipynb Extraction exploration
â”‚   â”œâ”€â”€ 02_transform_data.ipynb Transformation exploration
â”‚   â””â”€â”€ 03_load_data.ipynb    Load & validation exploration
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract.py            Extract layer
â”‚   â”œâ”€â”€ transform.py          Transform layer
â”‚   â”œâ”€â”€ load.py               Load layer
â”‚   â””â”€â”€ convert_to_json.py    Utility script
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore


---

### ğŸ”¹ BLOK 4 â€” ETL Pipeline Breakdown

```md
## ğŸ”„ ETL Pipeline Breakdown

### Extract â€” Raw Data

Cryptocurrency market data is retrieved from a public API and stored in **raw JSON format** without modification to preserve data integrity.

Each extraction generates a **timestamped raw file** to ensure traceability and reproducibility:

crypto_raw_YYYYMMDD_HHMMSS.json


---

### Transform â€” Processed Data

The transformation stage:

- Parses nested API responses  
- Normalizes and standardizes the schema  
- Converts timestamps into readable datetime formats  
- Handles missing or invalid values  

The transformation logic is **robust, reusable, and modular**, producing timestamped processed datasets.

---

### Load â€” Final Dataset

The load stage performs lightweight validation and sorting before producing an **analytics-ready dataset**.

This dataset serves as the **single source of truth** for downstream analysis and dashboard visualization.

## ğŸ“Š Streamlit Dashboard

The Streamlit dashboard enables interactive exploration of cryptocurrency data, including:

- Time-series price visualization  
- Latest market value display  
- Clean and minimal analytical interface  

Run the dashboard locally:

```bash
streamlit run dashboard/app.py

If the dashboard loads successfully, the ETL pipeline is fully operational.


---

### ğŸ”¹ BLOK 6 â€” Setup Instructions

```md
## âš™ï¸ Setup Instructions

Create and activate a virtual environment, then install dependencies:

```bash
python -m venv venv
source venv/Scripts/activate   # Windows Git Bash
pip install -r requirements.txt


---

### ğŸ”¹ BLOK 7 â€” Run Pipeline

```md
## â–¶ï¸ Run the Pipeline (Scripts)

Execute the ETL pipeline sequentially to generate fresh JSON data:

```bash
python scripts/extract.py
python scripts/transform.py
python scripts/load.py


---

### ğŸ”¹ BLOK 8 â€” GitHub Actions Automation

```md
## ğŸ¤– Automation with GitHub Actions

The ETL pipeline is fully automated using **GitHub Actions**, featuring:

- Scheduled execution (every 6 hours)  
- Manual trigger via `workflow_dispatch`  
- Automated dependency installation  
- End-to-end ETL execution  
- Automatic commit of updated datasets  

Workflow configuration:

```text
.github/workflows/etl_pipeline.yml


---

### ğŸ”¹ BLOK 9 â€” Notebook Usage

```md
## ğŸ““ Notebook Usage

The `notebooks/` directory mirrors the production ETL logic and is intended for:

- Exploration  
- Debugging  
- Documentation  
- Demonstration  

Recommended execution order:

1. `01_extract_data.ipynb`  
2. `02_transform_data.ipynb`  
3. `03_load_data.ipynb`

## ğŸ§  Key Skills Demonstrated

- API data ingestion  
- JSON-based ETL pipelines  
- Data cleaning and transformation  
- Modular Python scripting  
- CI/CD automation with GitHub Actions  
- Streamlit dashboard development  
- Reproducible and production-ready project structure  
- Professional technical documentation  

---

## ğŸš€ Future Improvements

Planned enhancements include:

- Multi-cryptocurrency support  
- Database integration (PostgreSQL / SQLite)  
- Advanced analytics (returns, volatility, indicators)  
- Workflow orchestration with Airflow  
- Containerization with Docker  
- Cloud deployment (Streamlit Cloud / AWS)  

---

## ğŸ“œ License

This project is open-source and intended for **educational and portfolio use**.

---

## ğŸ™Œ Author

Built as part of a **Data Engineering / Data Science portfolio** to demonstrate practical, real-world ETL automation and analytics workflows.